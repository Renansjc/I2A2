"""
Agente Aut√¥nomo para An√°lise Explorat√≥ria de Dados (EDA)
Autor: [Seu Nome] - VERS√ÉO AJUSTADA
Framework: OpenAI + Streamlit
Altera√ß√µes menores: prompt do LLM melhorado; suporte √† an√°lise de distribui√ß√£o em m√∫ltiplas colunas; tratamento da coluna Time quando √© num√©rica; corre√ß√µes de UI/slider e prote√ß√µes contra respostas vazias do LLM.
"""

import os
import json
import hashlib
import logging
import traceback
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List, Any, Tuple
import warnings

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

from openai import OpenAI

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Suprimir warnings desnecess√°rios
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

try:
    from dotenv import load_dotenv
    load_dotenv('config.env')
except:
    pass

# ============================================================================
# CONFIGURA√á√ïES
# ============================================================================
PAGE_CONFIG = {
    "page_title": "EDA Agent Pro",
    "page_icon": "üìä",
    "layout": "wide",
    "initial_sidebar_state": "expanded"
}

# Configura√ß√µes do sistema
MAX_FILE_SIZE_MB = 200
MAX_ROWS_PREVIEW = 100
MAX_COLUMNS_FOR_ANALYSIS = 50
ANALYSIS_MEMORY_DIR = Path(".eda_sessions")
ANALYSIS_MEMORY_DIR.mkdir(exist_ok=True)

# Configura√ß√µes de an√°lise
CORRELATION_THRESHOLD = 0.5
ANOMALY_ZSCORE_THRESHOLD = 3
ANOMALY_IQR_MULTIPLIER = 1.5
PCA_COMPONENTS = 0.95  # Vari√¢ncia explicada m√≠nima

# Mensagens de erro padronizadas
ERROR_MESSAGES = {
    "file_too_large": f"Arquivo muito grande. M√°ximo permitido: {MAX_FILE_SIZE_MB}MB",
    "invalid_csv": "Arquivo CSV inv√°lido ou corrompido",
    "empty_dataset": "Dataset est√° vazio ou n√£o possui dados v√°lidos",
    "insufficient_data": "Dados insuficientes para an√°lise",
    "api_error": "Erro na comunica√ß√£o com a API OpenAI",
    "analysis_error": "Erro durante an√°lise dos dados"
}

# ============================================================================
# CLASSE PRINCIPAL: DataAnalyzer
# ============================================================================
class DataAnalyzer:
    """Analisador de dados com capacidades de EDA e gera√ß√£o de insights"""
    
    def __init__(self, dataframe: pd.DataFrame, session_id: str):
        self.df = dataframe
        self.session_id = session_id
        self.insights_history = []
        self._validate_dataframe()
        self.metadata = self._extract_metadata()
        
    def _validate_dataframe(self):
        """Valida se o dataframe √© adequado para an√°lise"""
        if self.df.empty:
            raise ValueError(ERROR_MESSAGES["empty_dataset"])
        
        if len(self.df.columns) > MAX_COLUMNS_FOR_ANALYSIS:
            logger.warning(f"Dataset tem {len(self.df.columns)} colunas. Limitando an√°lise √†s primeiras {MAX_COLUMNS_FOR_ANALYSIS}")
            self.df = self.df.iloc[:, :MAX_COLUMNS_FOR_ANALYSIS]
        
        if len(self.df) < 5:
            raise ValueError(ERROR_MESSAGES["insufficient_data"])
        
        # Verificar se h√° pelo menos uma coluna num√©rica
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            logger.warning("Nenhuma coluna num√©rica encontrada. An√°lises quantitativas ser√£o limitadas.")
        
    def _extract_metadata(self) -> Dict[str, Any]:
        """Extrai metadados do dataset sem enviar dados completos"""
        try:
            numeric_columns = self.df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_columns = self.df.select_dtypes(include=['object', 'category']).columns.tolist()
            datetime_columns = self.df.select_dtypes(include=['datetime64']).columns.tolist()
            
            metadata = {
                "shape": self.df.shape,
                "columns": self.df.columns.tolist(),
                "dtypes": {col: str(dtype) for col, dtype in self.df.dtypes.items()},
                "numeric_cols": numeric_columns,
                "categorical_cols": categorical_columns,
                "datetime_cols": datetime_columns,
                "missing_values": self.df.isnull().sum().to_dict(),
                "missing_percentage": (self.df.isnull().sum() / len(self.df) * 100).to_dict(),
                "sample_stats": {},
                "data_quality": self._assess_data_quality()
            }
            
            # Estat√≠sticas b√°sicas para colunas num√©ricas
            for col in numeric_columns[:15]:  # Aumentado para 15
                try:
                    col_data = self.df[col].dropna()
                    if len(col_data) > 0:
                        metadata["sample_stats"][col] = {
                            "mean": float(col_data.mean()),
                            "std": float(col_data.std()),
                            "min": float(col_data.min()),
                            "max": float(col_data.max()),
                            "median": float(col_data.median()),
                            "q25": float(col_data.quantile(0.25)),
                            "q75": float(col_data.quantile(0.75)),
                            "skewness": float(stats.skew(col_data)),
                            "kurtosis": float(stats.kurtosis(col_data))
                        }
                except Exception as e:
                    logger.warning(f"Erro ao calcular estat√≠sticas para {col}: {e}")
            
            return metadata
        except Exception as e:
            logger.error(f"Erro ao extrair metadados: {e}")
            raise ValueError(f"Erro ao processar metadados: {str(e)}")
    
    def _assess_data_quality(self) -> Dict[str, Any]:
        """Avalia a qualidade dos dados"""
        total_cells = self.df.size
        missing_cells = self.df.isnull().sum().sum()
        duplicate_rows = self.df.duplicated().sum()
        
        return {
            "completeness": (total_cells - missing_cells) / total_cells * 100,
            "missing_cells": int(missing_cells),
            "duplicate_rows": int(duplicate_rows),
            "duplicate_percentage": duplicate_rows / len(self.df) * 100,
            "total_cells": int(total_cells)
        }
    
    def get_summary(self) -> Dict[str, Any]:
        """Retorna resumo geral do dataset"""
        return {
            "shape": self.metadata['shape'],
            "columns": len(self.metadata['columns']),
            "numeric_cols": len(self.metadata['numeric_cols']),
            "categorical_cols": len(self.metadata['categorical_cols']),
            "missing_values": sum(self.metadata['missing_values'].values()),
            "sample_stats": self.metadata['sample_stats']
        }
    
    def compute_correlation_analysis(self) -> Dict[str, Any]:
        """An√°lise de correla√ß√£o entre vari√°veis num√©ricas"""
        try:
            numeric_cols = self.metadata['numeric_cols']
            
            if len(numeric_cols) < 2:
                return {"error": "Necess√°rio pelo menos 2 colunas num√©ricas"}
            
            numeric_df = self.df[numeric_cols].dropna()
            
            if len(numeric_df) < 10:
                return {"error": "Dados insuficientes ap√≥s remo√ß√£o de valores faltantes"}
            
            corr_matrix = numeric_df.corr()
            
            # Encontrar correla√ß√µes mais fortes
            strong_correlations = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) > CORRELATION_THRESHOLD:
                        strong_correlations.append({
                            "var1": corr_matrix.columns[i],
                            "var2": corr_matrix.columns[j],
                            "correlation": float(corr_val),
                            "strength": "forte" if abs(corr_val) > 0.7 else "moderada"
                        })
            
            # Detectar multicolinearidade
            multicollinearity = self._detect_multicollinearity(corr_matrix)
            
            return {
                "correlation_matrix": corr_matrix,
                "strong_correlations": sorted(strong_correlations, 
                                             key=lambda x: abs(x['correlation']), 
                                             reverse=True)[:15],
                "multicollinearity": multicollinearity,
                "correlation_summary": {
                    "total_pairs": len(numeric_cols) * (len(numeric_cols) - 1) // 2,
                    "strong_pairs": len(strong_correlations),
                    "avg_correlation": float(corr_matrix.abs().mean().mean())
                }
            }
        except Exception as e:
            logger.error(f"Erro na an√°lise de correla√ß√£o: {e}")
            return {"error": f"Erro na an√°lise de correla√ß√£o: {str(e)}"}
    
    def _detect_multicollinearity(self, corr_matrix: pd.DataFrame) -> Dict[str, Any]:
        """Detecta problemas de multicolinearidade"""
        try:
            # Calcular VIF aproximado usando correla√ß√£o
            high_corr_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = abs(corr_matrix.iloc[i, j])
                    if corr_val > 0.8:
                        high_corr_pairs.append({
                            "var1": corr_matrix.columns[i],
                            "var2": corr_matrix.columns[j],
                            "correlation": float(corr_val)
                        })
            
            return {
                "high_correlation_pairs": high_corr_pairs,
                "has_multicollinearity": len(high_corr_pairs) > 0,
                "severity": "alta" if len(high_corr_pairs) > 3 else "moderada" if len(high_corr_pairs) > 0 else "baixa"
            }
        except Exception as e:
            logger.warning(f"Erro ao detectar multicolinearidade: {e}")
            return {"error": str(e)}
    
    def detect_anomalies(self, column: str, method: str = "iqr") -> Dict[str, Any]:
        """Detecta anomalias em uma coluna espec√≠fica"""
        try:
            if column not in self.metadata['numeric_cols']:
                return {"error": f"Coluna {column} n√£o √© num√©rica"}
            
            data = self.df[column].dropna()
            
            if len(data) < 10:
                return {"error": "Dados insuficientes para detec√ß√£o de anomalias"}
            
            anomalies = None
            threshold = ""
            anomaly_indices = []
            
            if method == "zscore":
                z_scores = np.abs((data - data.mean()) / data.std())
                anomaly_mask = z_scores > ANOMALY_ZSCORE_THRESHOLD
                anomalies = data[anomaly_mask]
                anomaly_indices = data[anomaly_mask].index.tolist()
                threshold = f"Z-score > {ANOMALY_ZSCORE_THRESHOLD}"
                
            elif method == "iqr":
                Q1 = data.quantile(0.25)
                Q3 = data.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - ANOMALY_IQR_MULTIPLIER * IQR
                upper_bound = Q3 + ANOMALY_IQR_MULTIPLIER * IQR
                anomaly_mask = (data < lower_bound) | (data > upper_bound)
                anomalies = data[anomaly_mask]
                anomaly_indices = data[anomaly_mask].index.tolist()
                threshold = f"IQR: [{lower_bound:.2f}, {upper_bound:.2f}]"
                
            elif method == "isolation_forest":
                # Implementa√ß√£o simples de Isolation Forest
                from sklearn.ensemble import IsolationForest
                iso_forest = IsolationForest(contamination=0.1, random_state=42)
                anomaly_labels = iso_forest.fit_predict(data.values.reshape(-1, 1))
                anomaly_mask = anomaly_labels == -1
                anomalies = data[anomaly_mask]
                anomaly_indices = data[anomaly_mask].index.tolist()
                threshold = "Isolation Forest (contamination=0.1)"
            
            if anomalies is None:
                return {"error": f"M√©todo {method} n√£o implementado"}
            
            return {
                "column": column,
                "total_values": len(data),
                "anomalies_count": len(anomalies),
                "anomalies_percentage": (len(anomalies) / len(data)) * 100,
                "method": method,
                "threshold": threshold,
                "anomalies_sample": anomalies.head(10).tolist(),
                "anomaly_indices": anomaly_indices[:20],  # Limitar para performance
                "statistics": {
                    "mean": float(data.mean()),
                    "std": float(data.std()),
                    "min": float(data.min()),
                    "max": float(data.max())
                }
            }
        except Exception as e:
            logger.error(f"Erro na detec√ß√£o de anomalias: {e}")
            return {"error": f"Erro na detec√ß√£o de anomalias: {str(e)}"}
    
    def temporal_analysis(self, time_col: str, value_col: str) -> Dict[str, Any]:
        """An√°lise temporal de uma s√©rie
        Nota: trata automaticamente quando a coluna de tempo √© num√©rica (por exemplo: segundos desde o in√≠cio).
        **Importante:** quando a coluna for num√©rica, INTERPRETAR COMO SEGUNDOS desde a primeira transa√ß√£o e **N√ÉO** converter para Unix epoch (1970-01-01). O eixo dever√° mostrar segundos (ou tempo decorrido) em vez de datas de 1970.
        """
        try:
            if time_col not in self.df.columns or value_col not in self.metadata['numeric_cols']:
                return {"error": "Colunas inv√°lidas"}

            df_temp = self.df[[time_col, value_col]].copy()
            # Se a coluna de tempo for num√©rica, interpretar como segundos desde o in√≠cio (n√£o usar epoch unix)
            if pd.api.types.is_numeric_dtype(df_temp[time_col]):
                df_temp['__time_seconds'] = df_temp[time_col].astype(float)
                # calcular tempo decorrido a partir do primeiro evento (em segundos)
                df_temp['__time_elapsed'] = df_temp['__time_seconds'] - df_temp['__time_seconds'].min()
                df_temp = df_temp.dropna(subset=['__time_elapsed'])
                df_temp = df_temp.sort_values('__time_elapsed')
                time_use = '__time_elapsed'
                time_label = 'Seconds since first transaction'
                is_numeric_time = True
            else:
                df_temp[time_col] = pd.to_datetime(df_temp[time_col], errors='coerce')
                df_temp = df_temp.dropna(subset=[time_col])
                df_temp = df_temp.sort_values(time_col)
                time_use = time_col
                time_label = time_col
                is_numeric_time = False

            if len(df_temp) < 10:
                return {"error": "Dados insuficientes para an√°lise temporal"}

            # Tend√™ncia (m√©dia m√≥vel)
            window_size = min(30, max(3, len(df_temp)//10))
            df_temp['moving_avg'] = df_temp[value_col].rolling(window=window_size, min_periods=1).mean()

            # Regress√£o simples sobre o √≠ndice (mantendo a unidade de tempo consistente quando for num√©rico)
            if is_numeric_time:
                x = df_temp[time_use].values.astype(float)
            else:
                x = np.arange(len(df_temp))
            y = df_temp[value_col].values
            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)

            # Sazonalidade simples (apenas faz sentido quando time n√£o √© apenas segundos decorrido ou quando h√° datetime real)
            seasonal_pattern = None
            if (not is_numeric_time) and len(df_temp) > 50:
                try:
                    df_temp['month'] = df_temp[time_use].dt.month
                    monthly_avg = df_temp.groupby('month')[value_col].mean()
                    seasonal_pattern = monthly_avg.to_dict()
                except Exception:
                    seasonal_pattern = None

            return {
                "has_trend": True,
                "data": df_temp,
                "start": float(df_temp[time_use].min()) if is_numeric_time else str(df_temp[time_use].min()),
                "end": float(df_temp[time_use].max()) if is_numeric_time else str(df_temp[time_use].max()),
                "time_label": time_label,
                "mean_value": float(df_temp[value_col].mean()),
                "trend_analysis": {
                    "slope": float(slope),
                    "r_squared": float(r_value**2),
                    "p_value": float(p_value),
                    "trend_direction": "crescente" if slope > 0 else "decrescente" if slope < 0 else "est√°vel"
                },
                "seasonal_pattern": seasonal_pattern,
                "data_points": len(df_temp)
            }
        except Exception as e:
            logger.error(f"Erro na an√°lise temporal: {e}")
            return {"error": f"Erro na an√°lise temporal: {str(e)}"}

    def analyze_distribution(self, column: str) -> Dict[str, Any]:
        """An√°lise detalhada de distribui√ß√£o de uma vari√°vel"""
        try:
            if column not in self.metadata['numeric_cols']:
                return {"error": f"Coluna {column} n√£o √© num√©rica"}
            
            data = self.df[column].dropna()
            if len(data) < 10:
                return {"error": "Dados insuficientes para an√°lise de distribui√ß√£o"}
            
            # Teste de normalidade
            shapiro_stat, shapiro_p = stats.shapiro(data) if len(data) <= 5000 else (None, None)
            ks_stat, ks_p = stats.kstest(data, 'norm', args=(data.mean(), data.std()))
            
            # An√°lise de assimetria e curtose
            skewness = stats.skew(data)
            kurtosis = stats.kurtosis(data)
            
            # Classifica√ß√£o da distribui√ß√£o
            distribution_type = self._classify_distribution(skewness, kurtosis)
            
            return {
                "column": column,
                "distribution_type": distribution_type,
                "statistics": {
                    "mean": float(data.mean()),
                    "median": float(data.median()),
                    "mode": float(data.mode().iloc[0]) if len(data.mode()) > 0 else None,
                    "std": float(data.std()),
                    "variance": float(data.var()),
                    "skewness": float(skewness),
                    "kurtosis": float(kurtosis),
                    "range": float(data.max() - data.min()),
                    "iqr": float(data.quantile(0.75) - data.quantile(0.25))
                },
                "normality_tests": {
                    "shapiro_wilk": {"statistic": float(shapiro_stat) if shapiro_stat else None, 
                                       "p_value": float(shapiro_p) if shapiro_p else None},
                    "kolmogorov_smirnov": {"statistic": float(ks_stat), "p_value": float(ks_p)}
                },
                "percentiles": {
                    "p5": float(data.quantile(0.05)),
                    "p25": float(data.quantile(0.25)),
                    "p50": float(data.quantile(0.50)),
                    "p75": float(data.quantile(0.75)),
                    "p95": float(data.quantile(0.95))
                }
            }
        except Exception as e:
            logger.error(f"Erro na an√°lise de distribui√ß√£o: {e}")
            return {"error": f"Erro na an√°lise de distribui√ß√£o: {str(e)}"}
    
    def _classify_distribution(self, skewness: float, kurtosis: float) -> str:
        """Classifica o tipo de distribui√ß√£o baseado em assimetria e curtose"""
        if abs(skewness) < 0.5 and abs(kurtosis) < 0.5:
            return "normal"
        elif skewness > 0.5:
            return "positivamente assim√©trica"
        elif skewness < -0.5:
            return "negativamente assim√©trica"
        elif kurtosis > 0.5:
            return "leptoc√∫rtica (picos altos)"
        elif kurtosis < -0.5:
            return "platic√∫rtica (picos baixos)"
        else:
            return "distribui√ß√£o mista"
    
    def perform_pca_analysis(self) -> Dict[str, Any]:
        """An√°lise de Componentes Principais"""
        try:
            numeric_cols = self.metadata['numeric_cols']
            if len(numeric_cols) < 3:
                return {"error": "PCA requer pelo menos 3 vari√°veis num√©ricas"}
            
            # Preparar dados
            data = self.df[numeric_cols].dropna()
            if len(data) < 10:
                return {"error": "Dados insuficientes para PCA"}
            
            # Normalizar dados
            scaler = StandardScaler()
            data_scaled = scaler.fit_transform(data)
            
            # PCA
            pca = PCA(n_components=PCA_COMPONENTS)
            pca_result = pca.fit_transform(data_scaled)
            
            # Componentes principais
            components = pd.DataFrame(
                pca.components_.T,
                columns=[f'PC{i+1}' for i in range(pca.components_.shape[0])],
                index=numeric_cols
            )
            
            return {
                "explained_variance_ratio": pca.explained_variance_ratio_.tolist(),
                "cumulative_variance": np.cumsum(pca.explained_variance_ratio_).tolist(),
                "n_components": pca.n_components_,
                "components": components.to_dict(),
                "total_variance_explained": float(np.sum(pca.explained_variance_ratio_)),
                "recommendation": self._pca_recommendation(pca.explained_variance_ratio_)
            }
        except Exception as e:
            logger.error(f"Erro na an√°lise PCA: {e}")
            return {"error": f"Erro na an√°lise PCA: {str(e)}"}
    
    def _pca_recommendation(self, explained_variance: np.ndarray) -> str:
        """Gera recomenda√ß√£o baseada na an√°lise PCA"""
        if len(explained_variance) <= 2:
            return "Dataset tem poucas dimens√µes para redu√ß√£o significativa"
        elif explained_variance[0] > 0.8:
            return "Primeira componente explica mais de 80% da vari√¢ncia - poss√≠vel redund√¢ncia nos dados"
        elif np.sum(explained_variance[:2]) > 0.7:
            return "Duas componentes principais explicam mais de 70% da vari√¢ncia"
        else:
            return "Dataset tem estrutura multidimensional complexa"
    
    def analyze_missing_data_patterns(self) -> Dict[str, Any]:
        """Analisa padr√µes de dados faltantes"""
        try:
            missing_data = self.df.isnull()
            
            # Padr√µes de missing data
            missing_patterns = {}
            for col in self.df.columns:
                if missing_data[col].sum() > 0:
                    missing_patterns[col] = {
                        "count": int(missing_data[col].sum()),
                        "percentage": float(missing_data[col].sum() / len(self.df) * 100),
                        "type": "MCAR" if missing_data[col].sum() < len(self.df) * 0.05 else "MAR"
                    }
            
            # Correla√ß√£o entre missing values
            missing_corr = missing_data.corr()
            
            return {
                "missing_by_column": missing_patterns,
                "total_missing": int(missing_data.sum().sum()),
                "missing_percentage": float(missing_data.sum().sum() / self.df.size * 100),
                "columns_with_missing": [col for col in self.df.columns if missing_data[col].sum() > 0],
                "complete_cases": int(len(self.df.dropna())),
                "complete_cases_percentage": float(len(self.df.dropna()) / len(self.df) * 100),
                "missing_correlation_matrix": missing_corr.to_dict() if len(missing_corr) > 0 else {}
            }
        except Exception as e:
            logger.error(f"Erro na an√°lise de missing data: {e}")
            return {"error": f"Erro na an√°lise de missing data: {str(e)}"}
    
    def suggest_analyses(self) -> List[Dict[str,str]]:
        """Sugere an√°lises baseadas nas caracter√≠sticas do dataset"""
        suggestions = []
        
        # Sugest√µes baseadas no tipo de dados
        if len(self.metadata['numeric_cols']) >= 2:
            suggestions.append({
                "analysis": "correlation",
                "title": "An√°lise de Correla√ß√£o",
                "description": "Identificar rela√ß√µes entre vari√°veis num√©ricas",
                "priority": "alta"
            })
        
        if len(self.metadata['numeric_cols']) >= 1:
            suggestions.append({
                "analysis": "distribution",
                "title": "An√°lise de Distribui√ß√µes",
                "description": "Verificar normalidade e caracter√≠sticas das distribui√ß√µes",
                "priority": "m√©dia"
            })
        
        if len(self.metadata['numeric_cols']) >= 3:
            suggestions.append({
                "analysis": "pca",
                "title": "An√°lise de Componentes Principais",
                "description": "Reduzir dimensionalidade e identificar padr√µes",
                "priority": "m√©dia"
            })
        
        # Sugest√µes baseadas na qualidade dos dados
        if self.metadata['data_quality']['missing_cells'] > 0:
            suggestions.append({
                "analysis": "missing_patterns",
                "title": "An√°lise de Dados Faltantes",
                "description": "Identificar padr√µes nos valores faltantes",
                "priority": "alta"
            })
        
        if self.metadata['data_quality']['duplicate_rows'] > 0:
            suggestions.append({
                "analysis": "duplicates",
                "title": "An√°lise de Duplicatas",
                "description": "Investigar registros duplicados",
                "priority": "m√©dia"
            })
        
        return suggestions


# ============================================================================
# CLASSE: MemoryManager
# ============================================================================
class MemoryManager:
    """Gerencia mem√≥ria persistente das an√°lises"""
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.memory_file = ANALYSIS_MEMORY_DIR / f"session_{session_id}.json"
        self.memory = self._load_memory()
    
    def _load_memory(self) -> Dict:
        """Carrega mem√≥ria existente"""
        if self.memory_file.exists():
            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        
        return {
            "queries": [],
            "insights": [],
            "conclusions": [],
            "created_at": datetime.now().isoformat(),
            "selected_model": None
        }
    
    def save_query(self, question: str, answer: str, chart_type: Optional[str] = None):
        """Salva uma query e resposta"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "answer": answer,
            "chart_type": chart_type
        }
        self.memory["queries"].append(entry)
        self._persist()
    
    def save_insight(self, insight: str, category: str):
        """Salva um insight descoberto"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "insight": insight,
            "category": category
        }
        self.memory["insights"].append(entry)
        self._persist()
    
    def save_conclusion(self, conclusion: str):
        """Salva conclus√£o geral da an√°lise"""
        self.memory["conclusions"].append({
            "timestamp": datetime.now().isoformat(),
            "text": conclusion
        })
        self._persist()

    def save_selected_model(self, model_name: str):
        """Persiste o modelo selecionado na mem√≥ria da sess√£o"""
        self.memory['selected_model'] = model_name
        self._persist()

    def get_context_summary(self) -> str:
        """Retorna resumo do contexto para a LLM"""
        num_queries = len(self.memory['queries'])
        # linha segura: a string √© fechada corretamente e cont√©m \n para nova linha
        summary = f"An√°lises anteriores realizadas: {num_queries}\n"

        if self.memory.get('insights'):
            summary += "\n√öltimos insights descobertos:\n"
            for insight in self.memory['insights'][-3:]:
                cat = insight.get('category', 'geral')
                text = insight.get('insight', '')
                # cada linha adicionada √© formatada e terminada com \\n
                summary += f"- [{cat}] {text}\n"

        return summary


    
    def _persist(self):
        """Persiste mem√≥ria em disco"""
        try:
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(self.memory, f, indent=2, ensure_ascii=False)
        except Exception as e:
            st.warning(f"Erro ao salvar mem√≥ria: {e}")


# ============================================================================
# CLASSE: LLMQueryProcessor
# ============================================================================
class LLMQueryProcessor:
    """Processa queries usando LLM"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        self.client = OpenAI(api_key=api_key)
        self.model = model
    
    def list_models(self) -> List[str]:
        """Lista modelos dispon√≠veis via API OpenAI. Retorna lista de nomes (strings)."""
        try:
            resp = self.client.models.list()
            models = []
            # A resposta pode variar de acordo com a vers√£o do SDK
            if hasattr(resp, 'data'):
                for m in resp.data:
                    # cada item geralmente tem um 'id' ou 'model'
                    if isinstance(m, dict):
                        name = m.get('id') or m.get('model')
                    else:
                        name = getattr(m, 'id', None) or getattr(m, 'model', None)
                    if name:
                        models.append(name)
            elif isinstance(resp, list):
                for m in resp:
                    if isinstance(m, dict):
                        name = m.get('id') or m.get('model')
                    else:
                        name = getattr(m, 'id', None) or getattr(m, 'model', None)
                    if name:
                        models.append(name)
            return sorted(list(set(models)))
        except Exception as e:
            logger.warning(f"N√£o foi poss√≠vel listar modelos: {e}")
            # fallback para lista padr√£o
            return [self.model]

    def set_model(self, model_name: str):
        """Define o modelo a ser usado nas chamadas subsequentes"""
        self.model = model_name

    def interpret_query(self, question: str, metadata: Dict, context: str) -> Dict[str, Any]:
        """Interpreta a pergunta e determina qual an√°lise executar"""
        
        # PROMPT MELHORADO: explicita o layout do dataset com √™nfase em datasets tipo 'creditcardfraud'
        system_prompt = """Voc√™ √© um assistente especialista em an√°lise de dados.
Analise a pergunta do usu√°rio e determine qual tipo de an√°lise deve ser executada.

Descri√ß√£o importante (quando presente):
- Time: n√∫mero de segundos desde a primeira transa√ß√£o (vari√°vel temporal cont√≠nua; **NUM√âRICA**). **NUNCA** interprete essa coluna como um timestamp Unix (1970-01-01). Quando "Time" for num√©rica, trate-a como segundos decorrido desde a primeira transa√ß√£o e indique que o eixo X deve estar em segundos (ou em formato decorrido leg√≠vel, ex: HH:MM:SS). 
- V1 a V28: componentes gerados por PCA (dimens√µes reduzidas)
- Amount: valor da transa√ß√£o (num√©rico)
- Class: indicador (0 = normal, 1 = fraudulenta)

Retorne APENAS um JSON v√°lido com a seguinte estrutura:
{
    "analysis_type": "correlation" | "anomaly" | "distribution" | "temporal" | "summary" | "conclusion" | "pca" | "missing_patterns" | "suggestions",
    "parameters": {
        "column": "nome_da_coluna" (se aplic√°vel),
        "columns": ["col1","col2"] (opcional ‚Äî quando pedir distribui√ß√µes de v√°rias colunas),
        "method": "iqr" | "zscore" | "isolation_forest" (para anomalias),
        "time_column": "coluna_temporal" (para an√°lise temporal),
        "value_column": "coluna_valor" (para an√°lise temporal)
    },
    "reasoning": "breve explica√ß√£o"
}

Instru√ß√µes adicionais:
- Se o usu√°rio perguntar sobre todas as distribui√ß√µes ou sobre 'vari√°veis num√©ricas', retorne parameters.columns com a lista de colunas num√©ricas ou a string "ALL_NUMERIC".
- Seja objetivo no campo 'reasoning'.
"""
        
        cols_sample = metadata['columns'][:20]
        num_cols_sample = metadata['numeric_cols'][:15]
        shape = metadata['shape']
        
        user_prompt = f"""Pergunta do usu√°rio: {question}

Metadados do dataset:
- Colunas dispon√≠veis: {cols_sample}
- Colunas num√©ricas: {num_cols_sample}
- Shape: {shape}

Contexto:
{context}

Determine qual an√°lise executar e preencha o JSON conforme instru√ß√µes do sistema."""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.2,
                response_format={"type": "json_object"}
            )
            
            # A API pode retornar diretamente o objeto JSON se configurada; tentar parse seguro
            raw = response.choices[0].message.content
            if isinstance(raw, dict):
                return raw
            try:
                return json.loads(raw)
            except Exception:
                return {"analysis_type": "summary", "parameters": {}, "reasoning": "Fallback: resposta n√£o-JSON"}
        except Exception as e:
            st.error(f"Erro ao interpretar query: {e}")
            return {
                "analysis_type": "summary",
                "parameters": {},
                "reasoning": f"Fallback devido a erro: {str(e)}"
            }
    
    def generate_insight(self, analysis_result: Dict, question: str) -> str:
        """Gera insight em linguagem natural"""
        
        system_prompt = """Voc√™ √© um analista de dados experiente.
Gere um insight claro em portugu√™s baseado nos resultados.
Seja conciso e objetivo (m√°ximo 200 palavras)."""
        
        result_str = json.dumps(analysis_result, indent=2, default=str)[:2000]
        
        user_prompt = f"""Pergunta: {question}

Resultados:
{result_str}

Gere um insight em portugu√™s."""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.5,
                max_tokens=400
            )
            
            return response.choices[0].message.content
        except Exception as e:
            return f"An√°lise conclu√≠da. Resultados: {str(analysis_result)[:300]}"
    
    def generate_conclusions(self, memory: Dict, metadata: Dict) -> str:
        """Gera conclus√µes gerais"""
        
        system_prompt = """Voc√™ √© um cientista de dados s√™nior.
Analise o hist√≥rico de an√°lises e gere conclus√µes gerais sobre o dataset.
Seja espec√≠fico, t√©cnico e objetivo."""
        
        insights_list = memory['insights'][-10:]
        insights_text = "\n".join([f"- {ins['insight']}" for ins in insights_list])
        
        num_rows = metadata['shape'][0]
        num_cols = metadata['shape'][1]
        num_numeric = len(metadata['numeric_cols'])
        
        user_prompt = f"""Dataset: {num_rows:,} linhas √ó {num_cols} colunas
Vari√°veis num√©ricas: {num_numeric}

Insights das an√°lises:
{insights_text}

Gere conclus√µes gerais em portugu√™s (m√°ximo 400 palavras)."""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.6,
                max_tokens=600
            )
            
            return response.choices[0].message.content
        except Exception as e:
            return f"Erro ao gerar conclus√µes: {str(e)}"


# ============================================================================
# CLASSE: ChartGenerator
# ============================================================================
class ChartGenerator:
    """Gera visualiza√ß√µes"""
    
    @staticmethod
    def plot_correlation_heatmap(corr_matrix: pd.DataFrame):
        """Gera heatmap de correla√ß√£o"""
        fig = go.Figure(data=go.Heatmap(
            z=corr_matrix.values,
            x=corr_matrix.columns,
            y=corr_matrix.columns,
            colorscale='RdBu',
            zmid=0,
            text=np.round(corr_matrix.values, 2),
            texttemplate='%{text}',
            textfont={"size": 9}
        ))
        
        height = min(600, 50 * len(corr_matrix.columns))
        fig.update_layout(title='Matriz de Correla√ß√£o', height=height)
        
        return fig
    
    @staticmethod
    def plot_distribution(data: pd.Series, column_name: str):
        """Gera gr√°fico de distribui√ß√£o"""
        fig = make_subplots(rows=1, cols=2, subplot_titles=('Histograma', 'Box Plot'))
        
        fig.add_trace(go.Histogram(x=data, name='Freq', nbinsx=30), row=1, col=1)
        fig.add_trace(go.Box(y=data, name=column_name), row=1, col=2)
        
        fig.update_layout(
            title=f'Distribui√ß√£o: {column_name}',
            height=400,
            showlegend=False
        )
        
        return fig
    
    @staticmethod
    def plot_temporal(df_temp: pd.DataFrame, time_col: str, value_col: str):
        """Gera gr√°fico temporal
        Observa√ß√£o: se a coluna temporal for num√©rica (tempo em segundos desde a primeira transa√ß√£o),
        o eixo X ser√° exibido em segundos (tempo decorrido) e N√ÉO ser√° convertido para Unix epoch (1970).
        """
        fig = go.Figure()

        # Detectar se eixo temporal √© num√©rico (segundos) ou datetime
        try:
            if pd.api.types.is_numeric_dtype(df_temp[time_col]):
                x_vals = df_temp[time_col]
                x_title = 'Seconds since first transaction'
            else:
                x_vals = df_temp[time_col]
                x_title = time_col
        except Exception:
            x_vals = df_temp[time_col]
            x_title = time_col

        fig.add_trace(go.Scatter(
            x=x_vals,
            y=df_temp[value_col],
            mode='lines',
            name='Valores'
        ))

        if 'moving_avg' in df_temp.columns:
            fig.add_trace(go.Scatter(
                x=x_vals,
                y=df_temp['moving_avg'],
                mode='lines',
                name='M√©dia M√≥vel',
                line=dict(dash='dash')
            ))

        fig.update_layout(
            title=f'An√°lise Temporal: {value_col}',
            xaxis_title=x_title,
            yaxis_title=value_col,
            hovermode='x unified',
            height=500
        )

        return fig


# ============================================================================
# INTERFACE STREAMLIT
# ============================================================================
def generate_session_id(file_content: bytes) -> str:
    """Gera ID √∫nico para sess√£o"""
    return hashlib.md5(file_content).hexdigest()[:12]


def validate_file_size(file_size_bytes: int) -> bool:
    """Valida se o arquivo est√° dentro do limite de tamanho"""
    max_size_bytes = MAX_FILE_SIZE_MB * 1024 * 1024
    return file_size_bytes <= max_size_bytes

def validate_csv_file(uploaded_file) -> Tuple[bool, str]:
    """Valida se o arquivo CSV √© v√°lido"""
    try:
        # Verificar tamanho
        file_size = len(uploaded_file.getvalue())
        if not validate_file_size(file_size):
            return False, ERROR_MESSAGES["file_too_large"]
        
        # Tentar ler o CSV
        uploaded_file.seek(0)  # Reset file pointer
        test_df = pd.read_csv(uploaded_file, nrows=5)
        uploaded_file.seek(0)  # Reset again
        
        if test_df.empty:
            return False, ERROR_MESSAGES["empty_dataset"]
        
        return True, "Arquivo v√°lido"
    except Exception as e:
        logger.error(f"Erro na valida√ß√£o do CSV: {e}")
        return False, ERROR_MESSAGES["invalid_csv"]

def main():
    st.set_page_config(**PAGE_CONFIG)
    
    st.title("ü§ñ Agente Aut√¥nomo de An√°lise Explorat√≥ria de Dados")
    st.markdown("*Framework: OpenAI GPT-4o-mini + Streamlit*")
    
    # Sidebar - configura√ß√£o e informa√ß√µes
    with st.sidebar:
        st.header("‚öôÔ∏è Configura√ß√£o")
        
        api_key = st.text_input(
            "OpenAI API Key",
            type="password",
            value=os.getenv("OPENAI_API_KEY", "")
        )
        
        if not api_key:
            st.warning("‚ö†Ô∏è Configure a API Key")
            st.stop()
        
        st.divider()
        
        # Informa√ß√µes do sistema
        st.header("üìä Informa√ß√µes do Sistema")
        st.info(f"""
        **Limites do Sistema:**
        - Tamanho m√°ximo: {MAX_FILE_SIZE_MB}MB
        - Colunas m√°ximas: {MAX_COLUMNS_FOR_ANALYSIS}
        - Preview m√°ximo: {MAX_ROWS_PREVIEW} linhas
        """)
        
        # Estat√≠sticas de sess√£o
        if 'session_stats' in st.session_state:
            st.header("üìà Estat√≠sticas da Sess√£o")
            stats = st.session_state.session_stats
            st.metric("An√°lises Realizadas", stats.get('analyses_count', 0))
            st.metric("Insights Gerados", stats.get('insights_count', 0))
            st.metric("Tempo de Sess√£o", f"{stats.get('session_time', 0):.1f} min")
    
    # Upload na √°rea principal
    st.divider()
    
    # Se√ß√£o de upload com valida√ß√£o
    col1, col2 = st.columns([3, 1])
    
    with col1:
        uploaded_file = st.file_uploader(
            "üìÅ Fa√ßa upload de um arquivo CSV para come√ßar",
            type=['csv'],
            help=f"Carregue um dataset em formato CSV (m√°ximo {MAX_FILE_SIZE_MB}MB)"
        )
    
    with col2:
        if uploaded_file:
            file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)
            st.metric("Tamanho", f"{file_size_mb:.1f} MB")
    
    if not uploaded_file:
        st.info("üëÜ Fa√ßa upload de um arquivo CSV para come√ßar")
        
        # Mostrar exemplos de perguntas
        st.subheader("üí° Exemplos de Perguntas")
        example_questions = [
            "Existe correla√ß√£o entre as vari√°veis num√©ricas?",
            "Quais s√£o os outliers na coluna Amount?",
            "Como est√° distribu√≠da a vari√°vel Amount?",
            "H√° padr√µes nos dados faltantes?",
            "Quais s√£o as principais componentes dos dados?",
            "Gere sugest√µes de an√°lises para este dataset"
        ]
        
        for i, question in enumerate(example_questions, 1):
            st.write(f"{i}. {question}")
        
        st.stop()
    
    # Validar arquivo
    is_valid, validation_message = validate_csv_file(uploaded_file)
    
    if not is_valid:
        st.error(f"‚ùå {validation_message}")
        st.stop()
    
    # Carregar dados com progress bar
    file_content = uploaded_file.getvalue()
    session_id = generate_session_id(file_content)
    
    @st.cache_data
    def load_data(content):
        return pd.read_csv(content)
    
    with st.spinner("Carregando dados..."):
        try:
            df = load_data(uploaded_file)
            
            # Inicializar estat√≠sticas de sess√£o
            if 'session_stats' not in st.session_state:
                st.session_state.session_stats = {
                    'analyses_count': 0,
                    'insights_count': 0,
                    'session_start': datetime.now(),
                    'session_time': 0
                }
            
            st.success(f"‚úÖ Dataset carregado: {df.shape[0]:,} linhas √ó {df.shape[1]} colunas")
            
            # Mostrar informa√ß√µes do dataset
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Linhas", f"{df.shape[0]:,}")
            with col2:
                st.metric("Colunas", df.shape[1])
            with col3:
                numeric_cols = len(df.select_dtypes(include=[np.number]).columns)
                st.metric("Num√©ricas", numeric_cols)
            with col4:
                missing_pct = (df.isnull().sum().sum() / df.size) * 100
                st.metric("Missing %", f"{missing_pct:.1f}%")
                
        except Exception as e:
            logger.error(f"Erro ao carregar dados: {e}")
            st.error(f"‚ùå Erro ao carregar: {str(e)}")
            with st.expander("Detalhes do erro"):
                st.code(traceback.format_exc())
            st.stop()
    
    # Inicializar componentes
    analyzer = DataAnalyzer(df, session_id)
    memory_mgr = MemoryManager(session_id)
    llm_processor = LLMQueryProcessor(api_key)
    chart_gen = ChartGenerator()

    # -------------------------------
    # Seletor de Modelo LLM (sidebar)
    # -------------------------------
    with st.sidebar:
        st.header("ü§ñ Modelo da LLM")
        # Carregar sele√ß√£o pr√©via (mem√≥ria)
        saved_model = memory_mgr.memory.get('selected_model', llm_processor.model)
        if 'selected_model' not in st.session_state:
            st.session_state['selected_model'] = saved_model

        # Fun√ß√£o para carregar modelos (faz GET via OpenAI client)
        def _fetch_models():
            try:
                models = llm_processor.list_models()
                st.session_state['available_models'] = models
                return models
            except Exception as e:
                st.error(f"Erro ao listar modelos: {e}")
                return [st.session_state['selected_model']]

        if st.button("üîÑ Carregar modelos dispon√≠veis"):
            with st.spinner("Buscando modelos..."):
                _fetch_models()

        available = st.session_state.get('available_models', [st.session_state['selected_model']])
        try:
            default_index = available.index(st.session_state['selected_model']) if st.session_state['selected_model'] in available else 0
        except Exception:
            default_index = 0
        selected_model = st.selectbox("Escolha um modelo para usar nas an√°lises:", options=available, index=default_index)

        if st.button("üíæ Salvar modelo selecionado"):
            st.session_state['selected_model'] = selected_model
            try:
                llm_processor.set_model(selected_model)
                memory_mgr.save_selected_model(selected_model)
                st.success(f"Modelo salvo: {selected_model}")
            except Exception as e:
                st.error(f"Erro ao salvar modelo: {e}")

        st.caption("Clique em 'Carregar modelos' para obter a lista de modelos dispon√≠veis via API OpenAI.")
    
    # Tabs melhoradas
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üìä Overview",
        "üîç An√°lise Interativa", 
        "üéØ An√°lises Sugeridas",
        "üí° Mem√≥ria",
        "üìù Conclus√µes"
    ])
    
    with tab1:
        st.subheader("üìä Resumo Estat√≠stico")
        
        # M√©tricas principais
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Linhas", f"{df.shape[0]:,}")
        with col2:
            st.metric("Colunas", df.shape[1])
        with col3:
            st.metric("Num√©ricas", len(analyzer.metadata['numeric_cols']))
        with col4:
            st.metric("Categ√≥ricas", len(analyzer.metadata['categorical_cols']))
        
        # Qualidade dos dados
        st.subheader("üîç Qualidade dos Dados")
        quality = analyzer.metadata['data_quality']
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Completude", f"{quality['completeness']:.1f}%")
        with col2:
            st.metric("Valores Faltantes", f"{quality['missing_cells']:,}")
        with col3:
            st.metric("Duplicatas", f"{quality['duplicate_rows']:,}")
        
        # Preview dos dados
        st.subheader("üëÄ Preview dos Dados")
        
        # Controles para preview (corrigido para n√£o ocorrer erro quando df tem poucas linhas)
        col1, col2 = st.columns([1, 3])
        with col1:
            max_rows = min(MAX_ROWS_PREVIEW, len(df))
            preview_rows = st.slider("Linhas para mostrar", 5, max_rows, min(20, max_rows))
            show_all_cols = st.checkbox("Mostrar todas as colunas", value=False)
        
        with col2:
            if show_all_cols:
                preview_df = df.head(preview_rows)
            else:
                preview_df = df.head(preview_rows).iloc[:, :10]  # Primeiras 10 colunas
            
            st.dataframe(preview_df, use_container_width=True)
        
        # Estat√≠sticas descritivas
        st.subheader("üìà Estat√≠sticas Descritivas")
        
        numeric_cols = analyzer.metadata['numeric_cols']
        if numeric_cols:
            st.dataframe(df[numeric_cols].describe(), use_container_width=True)
        else:
            st.info("Nenhuma coluna num√©rica encontrada para estat√≠sticas descritivas")
        
        # Informa√ß√µes sobre tipos de dados
        st.subheader("üìã Informa√ß√µes dos Tipos de Dados")
        
        dtype_info = pd.DataFrame({
            'Coluna': df.columns,
            'Tipo': df.dtypes.astype(str),
            'Valores √önicos': df.nunique(),
            'Valores Faltantes': df.isnull().sum(),
            '% Faltantes': (df.isnull().sum() / len(df) * 100).round(2)
        })
        
        st.dataframe(dtype_info, use_container_width=True)
    
    with tab2:
        st.subheader("üí¨ An√°lise Interativa")
        
        # Sugest√µes r√°pidas
        st.subheader("üöÄ Sugest√µes R√°pidas")
        quick_actions = st.columns(3)
        
        with quick_actions[0]:
            if st.button("üîó An√°lise de Correla√ß√£o", use_container_width=True):
                st.session_state.quick_question = "Existe correla√ß√£o entre as vari√°veis num√©ricas?"
        
        with quick_actions[1]:
            if st.button("üìä Distribui√ß√µes", use_container_width=True):
                st.session_state.quick_question = "Como est√£o distribu√≠das as vari√°veis num√©ricas?"
        
        with quick_actions[2]:
            if st.button("üîç Outliers", use_container_width=True):
                st.session_state.quick_question = "Quais s√£o os outliers nas vari√°veis num√©ricas?"
        
        # √Årea de pergunta
        st.subheader("üí≠ Sua Pergunta")
        
        # Usar pergunta r√°pida se dispon√≠vel
        default_question = st.session_state.get('quick_question', '')
        if default_question:
            st.session_state.quick_question = ''  # Limpar ap√≥s usar
        
        question = st.text_area(
            "Descreva o que voc√™ gostaria de analisar:",
            placeholder="Ex: Existe correla√ß√£o? Quais outliers em Amount? H√° padr√µes nos dados faltantes?",
            height=100,
            value=default_question
        )
        
        # Sele√ß√£o de colunas para an√°lises espec√≠ficas
        if analyzer.metadata['numeric_cols']:
            st.subheader("üéØ An√°lise Espec√≠fica")
            
            col1, col2 = st.columns(2)
            with col1:
                selected_column = st.selectbox(
                    "Selecione uma coluna num√©rica:",
                    options=[''] + analyzer.metadata['numeric_cols'],
                    help="Escolha uma coluna para an√°lises espec√≠ficas"
                )
            
            with col2:
                if selected_column:
                    analysis_type = st.selectbox(
                        "Tipo de an√°lise:",
                        options=['Distribui√ß√£o', 'Outliers', 'Estat√≠sticas'],
                        help="Tipo de an√°lise para a coluna selecionada"
                    )
                    
                    if st.button("üîç Analisar Coluna", use_container_width=True):
                        if analysis_type == 'Distribui√ß√£o':
                            question = f"Analise a distribui√ß√£o da coluna {selected_column}"
                        elif analysis_type == 'Outliers':
                            question = f"Identifique outliers na coluna {selected_column}"
                        else:
                            question = f"Mostre estat√≠sticas detalhadas da coluna {selected_column}"
        
        if st.button("üöÄ Analisar", type="primary"):
            if not question:
                st.warning("Digite uma pergunta")
            else:
                with st.spinner("Processando..."):
                    try:
                        context = memory_mgr.get_context_summary()
                        interpretation = llm_processor.interpret_query(
                            question, analyzer.metadata, context
                        )
                        
                        analysis_type = interpretation.get('analysis_type', 'summary')
                        params = interpretation.get('parameters', {}) or {}
                        st.info(f"üéØ An√°lise: **{analysis_type}**")
                        
                        result = None
                        chart = None
                        
                        # TRATAMENTO POR TIPO DE AN√ÅLISE
                        if analysis_type == 'summary':
                            result = analyzer.get_summary()
                        
                        elif analysis_type == 'correlation':
                            result = analyzer.compute_correlation_analysis()
                            if 'correlation_matrix' in result:
                                chart = chart_gen.plot_correlation_heatmap(
                                    result['correlation_matrix']
                                )
                        
                        elif analysis_type == 'anomaly':
                            col = params.get('column')
                            if col and col in analyzer.metadata['numeric_cols']:
                                method = params.get('method', 'iqr')
                                result = analyzer.detect_anomalies(col, method=method)
                            else:
                                result = {"error": "Especifique o par√¢metro 'column' para detec√ß√£o de anomalias."}
                        
                        elif analysis_type == 'distribution':
                            # Suporte a an√°lise de uma coluna, m√∫ltiplas colunas ou todas as num√©ricas
                            cols_param = params.get('column') or params.get('columns') or []
                            if isinstance(cols_param, str) and cols_param == 'ALL_NUMERIC':
                                cols = analyzer.metadata['numeric_cols']
                            elif isinstance(cols_param, str) and cols_param:
                                cols = [cols_param]
                            elif isinstance(cols_param, list) and len(cols_param) > 0:
                                cols = cols_param
                            else:
                                # fallback: todas as num√©ricas (limitado a 6 para performance)
                                cols = analyzer.metadata['numeric_cols'][:6]
                            
                            charts = []
                            distributions_summary = []
                            for c in cols:
                                if c in analyzer.metadata['numeric_cols']:
                                    distributions_summary.append(analyzer.analyze_distribution(c))
                                    charts.append(chart_gen.plot_distribution(df[c], c))
                            
                            if charts:
                                for fig in charts:
                                    st.plotly_chart(fig, use_container_width=True)
                                result = {"columns": cols, "status": "Gr√°ficos gerados", "count": len(charts)}
                            else:
                                result = {"error": "Nenhuma coluna v√°lida encontrada para distribui√ß√£o."}
                        
                        elif analysis_type == 'temporal':
                            params = interpretation.get('parameters', {})
                            time_col = params.get('time_column')
                            value_col = params.get('value_column')
                            if time_col and value_col:
                                result = analyzer.temporal_analysis(time_col, value_col)
                                if 'data' in result:
                                    # identificar qual coluna temporal foi usada (pode ter sido convertida internamente)
                                    time_used = '__time_dt' if '__time_dt' in result['data'].columns else time_col
                                    chart = chart_gen.plot_temporal(
                                        result['data'], time_used, value_col
                                    )
                            else:
                                result = {"error": "Especifique time_column e value_column para an√°lise temporal."}
                        
                        elif analysis_type == 'pca':
                            result = analyzer.perform_pca_analysis()
                            if 'components' in result:
                                # Criar gr√°fico de vari√¢ncia explicada
                                fig = go.Figure()
                                fig.add_trace(go.Bar(
                                    x=[f'PC{i+1}' for i in range(len(result['explained_variance_ratio']))],
                                    y=result['explained_variance_ratio'],
                                    name='Vari√¢ncia Explicada'
                                ))
                                fig.add_trace(go.Scatter(
                                    x=[f'PC{i+1}' for i in range(len(result['cumulative_variance']))],
                                    y=result['cumulative_variance'],
                                    mode='lines+markers',
                                    name='Vari√¢ncia Acumulada',
                                    yaxis='y2'
                                ))
                                fig.update_layout(
                                    title='An√°lise de Componentes Principais',
                                    xaxis_title='Componentes',
                                    yaxis_title='Vari√¢ncia Explicada',
                                    yaxis2=dict(title='Vari√¢ncia Acumulada', overlaying='y', side='right'),
                                    height=500
                                )
                                chart = fig
                        
                        elif analysis_type == 'missing_patterns':
                            result = analyzer.analyze_missing_data_patterns()
                            if 'missing_by_column' in result:
                                # Criar gr√°fico de missing data
                                missing_df = pd.DataFrame([
                                    {'Coluna': col, 'Valores Faltantes': info['count'], 'Percentual': info['percentage']}
                                    for col, info in result['missing_by_column'].items()
                                ])
                                fig = go.Figure()
                                fig.add_trace(go.Bar(
                                    x=missing_df['Coluna'],
                                    y=missing_df['Percentual'],
                                    text=missing_df['Valores Faltantes'],
                                    texttemplate='%{text}<br>%{y:.1f}%',
                                    textposition='outside'
                                ))
                                fig.update_layout(
                                    title='Padr√µes de Dados Faltantes',
                                    xaxis_title='Colunas',
                                    yaxis_title='Percentual de Valores Faltantes',
                                    height=400
                                )
                                chart = fig
                        
                        elif analysis_type == 'suggestions':
                            suggestions = analyzer.suggest_analyses()
                            result = {"suggestions": suggestions}
                            st.success("üí° Sugest√µes de An√°lises")
                            
                            for i, suggestion in enumerate(suggestions, 1):
                                priority_color = "üî¥" if suggestion['priority'] == 'alta' else "üü°" if suggestion['priority'] == 'm√©dia' else "üü¢"
                                st.write(f"{i}. {priority_color} **{suggestion['title']}**")
                                st.write(f"   {suggestion['description']}")
                                st.write("---")
                        
                        elif analysis_type == 'conclusion':
                            conclusion = llm_processor.generate_conclusions(
                                memory_mgr.memory, analyzer.metadata
                            )
                            st.success("üìã Conclus√µes Gerais")
                            st.write(conclusion)
                            memory_mgr.save_conclusion(conclusion)
                            result = {"done": True}
                        
                        # Gerar insight se houver resultado
                        if result and analysis_type not in ['conclusion', 'suggestions']:
                            if 'error' in result:
                                st.error(result['error'])
                            else:
                                insight = llm_processor.generate_insight(result, question)
                                st.success("‚ú® Resposta")
                                st.write(insight)
                                memory_mgr.save_query(question, insight, analysis_type)
                                memory_mgr.save_insight(insight, analysis_type)
                                
                                # Atualizar estat√≠sticas
                                if 'session_stats' in st.session_state:
                                    st.session_state.session_stats['analyses_count'] += 1
                                    st.session_state.session_stats['insights_count'] += 1
                        
                        # Mostrar gr√°fico (quando retornado como chart)
                        if chart:
                            st.plotly_chart(chart, use_container_width=True)
                        
                    except Exception as e:
                        logger.error(f"Erro durante an√°lise: {e}")
                        st.error(f"‚ùå Erro durante an√°lise: {str(e)}")
                        with st.expander("Ver detalhes do erro"):
                            st.code(traceback.format_exc())
    
    with tab3:
        st.subheader("üéØ An√°lises Sugeridas")
        
        # Obter sugest√µes
        suggestions = analyzer.suggest_analyses()
        
        if suggestions:
            st.info(f"üí° Encontramos {len(suggestions)} an√°lises recomendadas para seu dataset")
            
            # Agrupar por prioridade
            high_priority = [s for s in suggestions if s['priority'] == 'alta']
            medium_priority = [s for s in suggestions if s['priority'] == 'm√©dia']
            low_priority = [s for s in suggestions if s['priority'] == 'baixa']
            
            # Mostrar an√°lises de alta prioridade
            if high_priority:
                st.subheader("üî¥ Alta Prioridade")
                for i, suggestion in enumerate(high_priority, 1):
                    with st.expander(f"{i}. {suggestion['title']}"):
                        st.write(f"**Descri√ß√£o:** {suggestion['description']}")
                        st.write(f"**Tipo:** {suggestion['analysis']}")
                        
                        if st.button(f"Executar {suggestion['title']}", key=f"exec_{suggestion['analysis']}"):
                            # Simular execu√ß√£o da an√°lise
                            st.info(f"Executando {suggestion['title']}...")
                            # Aqui voc√™ pode chamar a an√°lise diretamente
            
            # Mostrar an√°lises de m√©dia prioridade
            if medium_priority:
                st.subheader("üü° M√©dia Prioridade")
                for i, suggestion in enumerate(medium_priority, 1):
                    with st.expander(f"{i}. {suggestion['title']}"):
                        st.write(f"**Descri√ß√£o:** {suggestion['description']}")
                        st.write(f"**Tipo:** {suggestion['analysis']}")
            
            # Mostrar an√°lises de baixa prioridade
            if low_priority:
                st.subheader("üü¢ Baixa Prioridade")
                for i, suggestion in enumerate(low_priority, 1):
                    with st.expander(f"{i}. {suggestion['title']}"):
                        st.write(f"**Descri√ß√£o:** {suggestion['description']}")
                        st.write(f"**Tipo:** {suggestion['analysis']}")
        else:
            st.info("Nenhuma an√°lise espec√≠fica sugerida para este dataset")
        
        # An√°lises autom√°ticas dispon√≠veis
        st.subheader("üîß An√°lises Autom√°ticas Dispon√≠veis")
        
        auto_analyses = [
            {"name": "An√°lise de Correla√ß√£o", "description": "Identifica rela√ß√µes entre vari√°veis num√©ricas", "icon": "üîó"},
            {"name": "An√°lise de Distribui√ß√µes", "description": "Verifica normalidade e caracter√≠sticas das distribui√ß√µes", "icon": "üìä"},
            {"name": "Detec√ß√£o de Outliers", "description": "Identifica valores at√≠picos usando m√∫ltiplos m√©todos", "icon": "üîç"},
            {"name": "An√°lise PCA", "description": "Reduz dimensionalidade e identifica componentes principais", "icon": "üìà"},
            {"name": "Padr√µes de Missing Data", "description": "Analisa padr√µes nos dados faltantes", "icon": "‚ùì"},
            {"name": "An√°lise Temporal", "description": "Identifica tend√™ncias e sazonalidade", "icon": "‚è∞"}
        ]
        
        cols = st.columns(2)
        for i, analysis in enumerate(auto_analyses):
            with cols[i % 2]:
                st.write(f"{analysis['icon']} **{analysis['name']}**")
                st.caption(analysis['description'])
    
    with tab4:
        st.subheader("üß† Mem√≥ria do Agente")
        
        num_queries = len(memory_mgr.memory['queries'])
        
        if num_queries > 0:
            st.write(f"**Total de an√°lises:** {num_queries}")
            
            for i, q in enumerate(reversed(memory_mgr.memory['queries'][-5:]), 1):
                preview = q['question'][:50]
                with st.expander(f"Query {num_queries - i + 1}: {preview}..."):
                    st.write(f"**Pergunta:** {q['question']}")
                    st.write(f"**Resposta:** {q['answer']}")
                    st.caption(f"üìÖ {q['timestamp']}")
        else:
            st.info("Nenhuma an√°lise realizada")
        
        st.divider()
        
        st.subheader("üí° Insights Acumulados")
        if memory_mgr.memory['insights']:
            for ins in memory_mgr.memory['insights'][-5:]:
                st.info(f"**[{ins['category']}]** {ins['insight']}")
        else:
            st.info("Nenhum insight gerado")
    
    with tab5:
        st.subheader("üìù Conclus√µes Finais")
        
        if st.button("üéØ Gerar Conclus√µes Gerais"):
            with st.spinner("Analisando hist√≥rico..."):
                try:
                    conclusion = llm_processor.generate_conclusions(
                        memory_mgr.memory, analyzer.metadata
                    )
                    st.success("‚úÖ Conclus√µes Geradas")
                    st.write(conclusion)
                    memory_mgr.save_conclusion(conclusion)
                except Exception as e:
                    st.error(f"Erro: {e}")
        
        if memory_mgr.memory['conclusions']:
            st.divider()
            st.subheader("Hist√≥rico")
            for conc in memory_mgr.memory['conclusions']:
                st.write(conc['text'])
                st.caption(f"üìÖ {conc['timestamp']}")
        else:
            st.info("Nenhuma conclus√£o gerada ainda")


if __name__ == "__main__":
    main()
